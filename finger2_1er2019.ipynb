{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "try: \n",
    "    type(sc)\n",
    "except NameError:\n",
    "    sc = pyspark.SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Los Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carga de los archivos csv\n",
    "df_events = spark.read.load(\"events.csv\",\n",
    "                    format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_installs=spark.read.load(\"installs.csv\",\n",
    "                    format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, application_id: string, ref_type: string, ref_hash: string, click_hash: string, device_countrycode: string, device_brand: string, device_model: string, session_user_agent: string, user_agent: string, event_uuid: string, kind: string, trans_id: string, ip_address: string, device_language: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_installs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, event_id: string, ref_type: string, ref_hash: string, application_id: string, device_countrycode: string, device_os_version: string, device_brand: string, device_model: string, device_city: string, session_user_agent: string, trans_id: string, user_agent: string, event_uuid: string, carrier: string, kind: string, device_os: string, connection_type: string, ip_address: string, device_language: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_events.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- created: timestamp (nullable = true)\n",
      " |-- application_id: integer (nullable = true)\n",
      " |-- ref_type: long (nullable = true)\n",
      " |-- ref_hash: long (nullable = true)\n",
      " |-- click_hash: string (nullable = true)\n",
      " |-- attributed: boolean (nullable = true)\n",
      " |-- implicit: boolean (nullable = true)\n",
      " |-- device_countrycode: long (nullable = true)\n",
      " |-- device_brand: decimal(18,-1) (nullable = true)\n",
      " |-- device_model: decimal(18,-1) (nullable = true)\n",
      " |-- session_user_agent: string (nullable = true)\n",
      " |-- user_agent: string (nullable = true)\n",
      " |-- event_uuid: string (nullable = true)\n",
      " |-- kind: string (nullable = true)\n",
      " |-- wifi: boolean (nullable = true)\n",
      " |-- trans_id: string (nullable = true)\n",
      " |-- ip_address: long (nullable = true)\n",
      " |-- device_language: decimal(18,-1) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_installs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- event_id: integer (nullable = true)\n",
      " |-- ref_type: long (nullable = true)\n",
      " |-- ref_hash: long (nullable = true)\n",
      " |-- application_id: integer (nullable = true)\n",
      " |-- attributed: boolean (nullable = true)\n",
      " |-- device_countrycode: long (nullable = true)\n",
      " |-- device_os_version: decimal(19,0) (nullable = true)\n",
      " |-- device_brand: decimal(18,-1) (nullable = true)\n",
      " |-- device_model: double (nullable = true)\n",
      " |-- device_city: decimal(19,0) (nullable = true)\n",
      " |-- session_user_agent: double (nullable = true)\n",
      " |-- trans_id: string (nullable = true)\n",
      " |-- user_agent: double (nullable = true)\n",
      " |-- event_uuid: string (nullable = true)\n",
      " |-- carrier: decimal(18,-1) (nullable = true)\n",
      " |-- kind: decimal(18,-1) (nullable = true)\n",
      " |-- device_os: decimal(17,-2) (nullable = true)\n",
      " |-- wifi: boolean (nullable = true)\n",
      " |-- connection_type: string (nullable = true)\n",
      " |-- ip_address: long (nullable = true)\n",
      " |-- device_language: decimal(19,0) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_events.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una vista temporal asociada a cada dataframe(Temporary View)\n",
    "df_events.createOrReplaceTempView(\"events\")\n",
    "df_installs.createOrReplaceTempView(\"installs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|wifi|\n",
      "+----+\n",
      "|null|\n",
      "|null|\n",
      "|null|\n",
      "|null|\n",
      "|null|\n",
      "|null|\n",
      "|null|\n",
      "|null|\n",
      "|null|\n",
      "|true|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT wifi from installs LIMIT 10').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis N°1: El total de aplicaciones que registraron eventos, en el set de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|total_app_events|\n",
      "+----------------+\n",
      "|             269|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cantApp_events=spark.sql(\"SELECT COUNT(DISTINCT(application_id)) as total_app_events\\\n",
    "                    FROM events\");\n",
    "cantApp_events.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+\n",
      "|application_id|cant_events|\n",
      "+--------------+-----------+\n",
      "|            66|     325696|\n",
      "|            64|     259084|\n",
      "|           145|     252431|\n",
      "|            63|     181555|\n",
      "|           103|     137513|\n",
      "+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "application_events= spark.sql(\"SELECT application_id, COUNT(*) as cant_events\\\n",
    "                                FROM events\\\n",
    "                                GROUP BY application_id\\\n",
    "                                ORDER BY cant_events DESC\");\n",
    "\n",
    "application_events.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis N°2: El total de aplicaciones que registraron instalaciones, en el set de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|total_app_install|\n",
      "+-----------------+\n",
      "|               31|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cantApp_installs=spark.sql(\"SELECT COUNT(DISTINCT(application_id)) as total_app_install\\\n",
    "                            FROM installs\");\n",
    "cantApp_installs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis N°3: El top 5 de aplicaciones que registraron instalaciones solamente desde wifi, indicando como resultado el identificador de la aplicación y la cantidad de instalaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+\n",
      "|application_id|cant_installs_app|\n",
      "+--------------+-----------------+\n",
      "|             9|              498|\n",
      "|            10|              372|\n",
      "|            16|              280|\n",
      "|             2|              164|\n",
      "|            15|               20|\n",
      "+--------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query='SELECT installs.application_id, COUNT(installs.wifi) as cant_installs_app\\\n",
    "        FROM installs\\\n",
    "        WHERE installs.wifi=\"true\"\\\n",
    "        GROUP BY application_id\\\n",
    "        ORDER BY cant_installs_app DESC\\\n",
    "        LIMIT 5'\n",
    "top5_installapp_wifi=spark.sql(query);\n",
    "top5_installapp_wifi.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis N°4: Encontrar los 5 dispositivos que realizaron mayor cantidad de eventos, indicando como resultado el identificador del dispositivo y la cantidad de eventos realizados.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+\n",
      "|          id_device|cant_events|\n",
      "+-------------------+-----------+\n",
      "|7823950631004872496|       2913|\n",
      "|7298478026707033340|       2822|\n",
      "|5034957474698180142|       2681|\n",
      "|5724169280369284055|       2524|\n",
      "|1706781657278990931|       2300|\n",
      "+-------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query2='SELECT events.ref_hash as id_device, COUNT(*) as cant_events\\\n",
    "        FROM events\\\n",
    "        GROUP BY ref_hash\\\n",
    "        ORDER BY cant_events DESC\\\n",
    "        LIMIT 5'\n",
    "top5_device_events=spark.sql(query2);\n",
    "top5_device_events.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analizar el output del Catalyst Optimizer en este punto. ¿Se realiza algun tipo de Join? En caso afirmativo, ¿Qué tipo de Join físico se realiza?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'GlobalLimit 5\n",
      "+- 'LocalLimit 5\n",
      "   +- 'Sort ['cant_events DESC NULLS LAST], true\n",
      "      +- 'Aggregate ['ref_hash], ['events.ref_hash AS id_device#2168, 'COUNT(1) AS cant_events#2169]\n",
      "         +- 'UnresolvedRelation `events`\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "id_device: bigint, cant_events: bigint\n",
      "GlobalLimit 5\n",
      "+- LocalLimit 5\n",
      "   +- Sort [cant_events#2169L DESC NULLS LAST], true\n",
      "      +- Aggregate [ref_hash#13L], [ref_hash#13L AS id_device#2168L, count(1) AS cant_events#2169L]\n",
      "         +- SubqueryAlias events\n",
      "            +- Relation[date#10,event_id#11,ref_type#12L,ref_hash#13L,application_id#14,attributed#15,device_countrycode#16L,device_os_version#17,device_brand#18,device_model#19,device_city#20,session_user_agent#21,trans_id#22,user_agent#23,event_uuid#24,carrier#25,kind#26,device_os#27,wifi#28,connection_type#29,ip_address#30L,device_language#31] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "GlobalLimit 5\n",
      "+- LocalLimit 5\n",
      "   +- Sort [cant_events#2169L DESC NULLS LAST], true\n",
      "      +- Aggregate [ref_hash#13L], [ref_hash#13L AS id_device#2168L, count(1) AS cant_events#2169L]\n",
      "         +- Project [ref_hash#13L]\n",
      "            +- Relation[date#10,event_id#11,ref_type#12L,ref_hash#13L,application_id#14,attributed#15,device_countrycode#16L,device_os_version#17,device_brand#18,device_model#19,device_city#20,session_user_agent#21,trans_id#22,user_agent#23,event_uuid#24,carrier#25,kind#26,device_os#27,wifi#28,connection_type#29,ip_address#30L,device_language#31] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[cant_events#2169L DESC NULLS LAST], output=[id_device#2168L,cant_events#2169L])\n",
      "+- *(2) HashAggregate(keys=[ref_hash#13L], functions=[count(1)], output=[id_device#2168L, cant_events#2169L])\n",
      "   +- Exchange hashpartitioning(ref_hash#13L, 200)\n",
      "      +- *(1) HashAggregate(keys=[ref_hash#13L], functions=[partial_count(1)], output=[ref_hash#13L, count#2175L])\n",
      "         +- *(1) FileScan csv [ref_hash#13L] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/home/sherly/Escritorio/orgaDatos1er2019/events.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<ref_hash:bigint>\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query2).explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1)__ Primeramente el query2 es parseado y como podemos observar nos muestra __+- 'UnresolvedRelation `events`__ osea relacion no resueltas aun,pues no sabe nada de los atributos en la query2; Luego en __== Analyzed Logical Plan ==__ se utiliza el dataframe original para el analisis, Una vez que el plan esta resuelto Catalyst comienza a optimizar este plan, generando un __== Optimized Logical Plan ==__.\n",
    "Y por ultimo una vez optimizado el plan Catalyst genera un conjunto de __== Physical Plan ==__ estos seran evaluados con un modelo de costos y el que posee mayor performance sera el plan usado.\n",
    "***\n",
    "__2)__ No se realiza ningun join.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
